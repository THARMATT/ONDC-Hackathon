{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0ae3358",
   "metadata": {},
   "source": [
    "# Data is getting Segmented based upon it's preferenses and history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b9c8d2",
   "metadata": {},
   "source": [
    "# DATA COLLECTION FROM DIFFERENT BUYER NPs AND APPLYING PRIVACY POLICIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7a6c531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to CSV files.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import os\n",
    "import base64\n",
    "import random\n",
    "\n",
    "# Define real product data\n",
    "real_product_data = {\n",
    "    'product_name': ['iPhone 13', 'Samsung Galaxy S21', 'MacBook Pro', 'Lenovo ThinkPad', 'Nike Air Max', 'Adidas Ultraboost', 'Harry Potter and the Sorcerer\\'s Stone', 'To Kill a Mockingbird', 'Amazon Echo Dot', 'Google Nest Mini'],\n",
    "    'category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Clothing', 'Clothing', 'Books', 'Books', 'Electronics', 'Electronics'],\n",
    "    'features': [['5G', 'A15 Bionic chip'], ['5G', 'Exynos 2100'], ['M1 chip', 'Retina display'], ['Intel Core processor', 'Windows 10'], ['Air Max cushioning', 'Mesh upper'], ['Boost cushioning', 'Primeknit upper'], ['Fantasy fiction', 'Wizardry'], ['Classic literature', 'Southern Gothic'], ['Voice control', 'Smart home integration'], ['Voice control', 'Smart home integration']],\n",
    "    'brand': ['Apple', 'Samsung', 'Apple', 'Lenovo', 'Nike', 'Adidas', 'J.K. Rowling', 'Harper Lee', 'Amazon', 'Google'],\n",
    "    'keywords': [['iPhone', 'Apple', 'Smartphone'], ['Samsung', 'Galaxy', 'Android'], ['MacBook', 'Apple', 'Laptop'], ['ThinkPad', 'Lenovo', 'Laptop'], ['Air Max', 'Nike', 'Sneakers'], ['Ultraboost', 'Adidas', 'Sneakers'], ['Harry Potter', 'Fantasy', 'Novel'], ['To Kill a Mockingbird', 'Classic', 'Novel'], ['Echo Dot', 'Amazon', 'Smart speaker'], ['Nest Mini', 'Google', 'Smart speaker']]\n",
    "}\n",
    "\n",
    "# Function to mask sensitive data (e.g., email, phone)\n",
    "def mask_data(data):\n",
    "    masked_data = \"\"\n",
    "    for char in data:\n",
    "        if char.isdigit():\n",
    "            masked_data += \"*\"\n",
    "        else:\n",
    "            masked_data += char\n",
    "    return masked_data\n",
    "\n",
    "# Function to encrypt sensitive data using XOR encryption and Base64 encoding\n",
    "def encrypt_data(data):\n",
    "    encryption_key = 0xAB  # Example encryption key (for demonstration purposes only)\n",
    "    data_bytes = data.encode('utf-8')\n",
    "    encrypted_bytes = bytes([byte ^ encryption_key for byte in data_bytes])\n",
    "    encrypted_data = base64.b64encode(encrypted_bytes).decode('utf-8')\n",
    "    return encrypted_data\n",
    "\n",
    "# Function to anonymize customer IDs using hashing\n",
    "def anonymize_customer_id(customer_id):\n",
    "    return hashlib.sha256(customer_id.encode()).hexdigest()\n",
    "\n",
    "# Function to generate synthetic customer data\n",
    "def generate_customer_data(num_customers, buyer_np_id):\n",
    "    customer_data = {\n",
    "        'customer_id': [anonymize_customer_id(f'Cust{i:04d}') for i in range(1, num_customers + 1)],\n",
    "        'age': np.random.randint(18, 65, num_customers),\n",
    "        'gender': np.random.choice(['Male', 'Female'], num_customers),\n",
    "        'location': [encrypt_data(location) for location in np.random.choice(['City', 'Suburb', 'Rural'], num_customers)],\n",
    "        'purchase_history': [np.random.choice(real_product_data['product_name'], size=np.random.randint(1, 4)).tolist() for _ in range(num_customers)],\n",
    "        'browsing_history': [np.random.choice(real_product_data['product_name'], size=np.random.randint(1, 4)).tolist() for _ in range(num_customers)],\n",
    "        'preferences': [np.random.choice(real_product_data['product_name'], size=np.random.randint(1, 4)).tolist() for _ in range(num_customers)]\n",
    "    }\n",
    "    df = pd.DataFrame(customer_data)\n",
    "    df['buyer_np_id'] = buyer_np_id  # Assign Buyer NP ID to customers\n",
    "    return df\n",
    "\n",
    "def generate_product_data(num_products, num_sellers):\n",
    "    product_data = {\n",
    "        'product_id': [f'P{i:04d}' for i in range(1, num_products + 1)],\n",
    "        'product_name': real_product_data['product_name'],\n",
    "        'category': real_product_data['category'],\n",
    "        'features': real_product_data['features'],\n",
    "        'brand': real_product_data['brand'],\n",
    "        'keywords': real_product_data['keywords'],\n",
    "        'seller_id': [f'Seller{random.randint(1, num_sellers):03d}' for _ in range(num_products)]  # Assign random seller IDs\n",
    "    }\n",
    "    return pd.DataFrame(product_data)\n",
    "\n",
    "\n",
    "# Generate synthetic product data with 10 products and 5 sellers\n",
    "num_products = 10\n",
    "num_sellers = 5\n",
    "product_data = generate_product_data(num_products, num_sellers)\n",
    "\n",
    "# Specify directory for saving files\n",
    "output_directory = \"C:\\\\Users\\\\NIGAM SHARMA\\\\Desktop\\\\fun\"\n",
    "\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Generate synthetic customer data for each Buyer NP\n",
    "num_customers_per_np = 10\n",
    "num_buyer_nps = 3\n",
    "customer_dfs = [generate_customer_data(num_customers_per_np, buyer_np_id) for buyer_np_id in range(1, num_buyer_nps + 1)]\n",
    "\n",
    "# Concatenate customer dataframes\n",
    "customer_data = pd.concat(customer_dfs)\n",
    "\n",
    "# Write customer data to CSV file\n",
    "customer_file_name = os.path.join(output_directory, 'customer_data.csv')\n",
    "customer_data.to_csv(customer_file_name, index=False)\n",
    "\n",
    "# Write product data to CSV file\n",
    "product_file_name = os.path.join(output_directory, 'product_data.csv')\n",
    "product_data.to_csv(product_file_name, index=False)\n",
    "\n",
    "print(\"Data saved to CSV files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5be85174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmented customer data saved to CSV file: C:\\Users\\NIGAM SHARMA\\Desktop\\fun\\segmented_customer_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\n",
    "\n",
    "# Load customer data from CSV file\n",
    "customer_data = pd.read_csv('C:\\\\Users\\\\NIGAM SHARMA\\\\Desktop\\\\fun\\\\customer_data.csv')\n",
    "\n",
    "# Define functions for data preprocessing and segmentation\n",
    "\n",
    "def preprocess_data(customer_data):\n",
    "    # Convert lists to strings\n",
    "    customer_data['purchase_history_str'] = customer_data['purchase_history'].apply(lambda x: ','.join(map(str, x)))\n",
    "    customer_data['browsing_history_str'] = customer_data['browsing_history'].apply(lambda x: ','.join(map(str, x)))\n",
    "    customer_data['preferences_str'] = customer_data['preferences'].apply(lambda x: ','.join(map(str, x)))\n",
    "\n",
    "    # One-hot encode categorical features\n",
    "    oh_encoder = OneHotEncoder()\n",
    "    purchase_history_encoded = oh_encoder.fit_transform(customer_data[['purchase_history_str']])\n",
    "    browsing_history_encoded = oh_encoder.fit_transform(customer_data[['browsing_history_str']])\n",
    "    preferences_encoded = oh_encoder.fit_transform(customer_data[['preferences_str']])\n",
    "\n",
    "    # Combine encoded features\n",
    "    encoded_features = pd.concat([pd.DataFrame(purchase_history_encoded.toarray()), \n",
    "                                  pd.DataFrame(browsing_history_encoded.toarray()),\n",
    "                                  pd.DataFrame(preferences_encoded.toarray())], axis=1)\n",
    "\n",
    "    return encoded_features\n",
    "\n",
    "def segment_customers(encoded_features):\n",
    "    # Perform K-means clustering on encoded features to create segments\n",
    "    kmeans = KMeans(n_clusters=5, random_state=0).fit(encoded_features)\n",
    "    return kmeans.labels_\n",
    "\n",
    "# Preprocess data\n",
    "encoded_features = preprocess_data(customer_data)\n",
    "\n",
    "# Segment customers\n",
    "segments = segment_customers(encoded_features)\n",
    "\n",
    "# Add segments to customer data\n",
    "customer_data['segment'] = segments\n",
    "\n",
    "# Specify directory for saving files\n",
    "output_directory = \"C:\\\\Users\\\\NIGAM SHARMA\\\\Desktop\\\\fun\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Create a DataFrame with customer IDs and their corresponding segments\n",
    "segmented_data = customer_data[['customer_id', 'segment']]\n",
    "\n",
    "# Write data to CSV file with customer IDs and segments\n",
    "output_file = os.path.join(output_directory, 'segmented_customer_data.csv')\n",
    "segmented_data.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"Segmented customer data saved to CSV file:\", output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778d9d2c",
   "metadata": {},
   "source": [
    "# Here the important data from the whole data is being collected to send it to the seller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ec9974e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer data prepared for sellers and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Function to prepare customer data for sharing with sellers\n",
    "def prepare_customer_data_for_sellers(customer_data):\n",
    "    # Select relevant columns for sharing\n",
    "    customer_data_for_sellers = customer_data[['customer_id', 'purchase_history', 'browsing_history', 'preferences']]\n",
    "    \n",
    "    return customer_data_for_sellers\n",
    "\n",
    "# Specify the full path to the CSV file\n",
    "csv_file_path = \"C:\\\\Users\\\\NIGAM SHARMA\\\\Desktop\\\\fun\\\\customer_data.csv\"\n",
    "\n",
    "# Load customer data from CSV file\n",
    "customer_data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Prepare customer data for sharing with sellers\n",
    "customer_data_for_sellers = prepare_customer_data_for_sellers(customer_data)\n",
    "\n",
    "# Specify directory for saving files\n",
    "output_directory = \"C:\\\\Users\\\\NIGAM SHARMA\\\\Desktop\\\\fun\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Write customer data to CSV file for sellers\n",
    "file_name = os.path.join(output_directory, 'customer_data_for_sellers.csv')\n",
    "customer_data_for_sellers.to_csv(file_name, index=False)\n",
    "\n",
    "print(\"Customer data prepared for sellers and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb6989a",
   "metadata": {},
   "source": [
    "# now we need to provide recommendations to each buyer NP equally so we are applying recommendation engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f304e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load customer data\n",
    "customer_data = pd.read_csv('C:\\\\Users\\\\NIGAM SHARMA\\\\Desktop\\\\fun\\\\customer_data.csv')\n",
    "\n",
    "\n",
    "# Load segment data\n",
    "segment_data = pd.read_csv('C:\\\\Users\\\\NIGAM SHARMA\\\\Desktop\\\\fun\\\\segmented_customer_data.csv')\n",
    "\n",
    "# Load product data\n",
    "product_data = pd.read_csv('C:\\\\Users\\\\NIGAM SHARMA\\\\Desktop\\\\fun\\\\product_data.csv')\n",
    "\n",
    "# Preprocessing customer data\n",
    "def preprocess_customer_data(customer_data):\n",
    "    # Convert categorical variables to numerical values if needed\n",
    "    customer_data['gender'] = pd.Categorical(customer_data['gender']).codes\n",
    "    return customer_data\n",
    "\n",
    "\n",
    "# preprocess segment data\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def preprocess_segment_data(segment_data):\n",
    "    \n",
    "    return segment_data\n",
    "\n",
    "\n",
    "# Preprocessing product data\n",
    "def preprocess_product_data(product_data):\n",
    "    # Convert categorical variables to numerical values if needed\n",
    "    return product_data\n",
    "\n",
    "# Apply preprocessing to each dataset\n",
    "customer_data = preprocess_customer_data(customer_data)\n",
    "segment_data = preprocess_segment_data(segment_data)\n",
    "product_data = preprocess_product_data(product_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642a3d56",
   "metadata": {},
   "source": [
    "# Analysing the customer product interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb19fa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_customer_product_interactions(customer_data):\n",
    "    # Extracting relevant columns for analysis\n",
    "    interactions_data = customer_data[['customer_id', 'purchase_history', 'browsing_history', 'preferences']]\n",
    "   \n",
    "    # Convert string representations of lists to actual lists\n",
    "    interactions_data['purchase_history'] = interactions_data['purchase_history'].apply(eval)\n",
    "    interactions_data['browsing_history'] = interactions_data['browsing_history'].apply(eval)\n",
    "    interactions_data['preferences'] = interactions_data['preferences'].apply(eval)\n",
    "    \n",
    "    # Perform further analysis or feature engineering as needed\n",
    "    \n",
    "    return interactions_data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have customer_data DataFrame with customer_id and purchase_history columns\n",
    "# Assuming you have product_data DataFrame with product_id column\n",
    "\n",
    "# Create an empty list to store interaction data\n",
    "interactions_data = []\n",
    "\n",
    "# Iterate over each row in the customer_data DataFrame\n",
    "for index, row in customer_data.iterrows():\n",
    "    # Extract customer_id and purchase_history\n",
    "    customer_id = row['customer_id']\n",
    "    purchase_history = row['purchase_history']\n",
    "    \n",
    "    # Convert purchase_history to a list if it's a string\n",
    "    if isinstance(purchase_history, str):\n",
    "        purchase_history = eval(purchase_history)\n",
    "    \n",
    "    # Iterate over each product in the purchase_history list\n",
    "    for product_id in purchase_history:\n",
    "        # Append a tuple (customer_id, product_id) to interactions_data\n",
    "        interactions_data.append((customer_id, product_id))\n",
    "\n",
    "# Create a DataFrame from interactions_data with columns customer_id and product_id\n",
    "interactions_data = pd.DataFrame(interactions_data, columns=['customer_id', 'product_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ede5f080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ae41df7579ec9ce4733b47727b2b62121e17810ccbf456...</td>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9da6aae167b6d83fce577cd60d32fa607206c327c7236c...</td>\n",
       "      <td>Amazon Echo Dot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9da6aae167b6d83fce577cd60d32fa607206c327c7236c...</td>\n",
       "      <td>Amazon Echo Dot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>becc3714c0f577a5c873dc028ccf5b05b6631ab807d799...</td>\n",
       "      <td>MacBook Pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>becc3714c0f577a5c873dc028ccf5b05b6631ab807d799...</td>\n",
       "      <td>Adidas Ultraboost</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_id             product_id\n",
       "0  ae41df7579ec9ce4733b47727b2b62121e17810ccbf456...  To Kill a Mockingbird\n",
       "1  9da6aae167b6d83fce577cd60d32fa607206c327c7236c...        Amazon Echo Dot\n",
       "2  9da6aae167b6d83fce577cd60d32fa607206c327c7236c...        Amazon Echo Dot\n",
       "3  becc3714c0f577a5c873dc028ccf5b05b6631ab807d799...            MacBook Pro\n",
       "4  becc3714c0f577a5c873dc028ccf5b05b6631ab807d799...      Adidas Ultraboost"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fac9627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   segment  customer_count\n",
      "0        0              19\n",
      "1        1               4\n",
      "2        3               3\n",
      "3        4               2\n",
      "4        2               2\n"
     ]
    }
   ],
   "source": [
    "def analyze_segments(segment_data):\n",
    "    # Count the number of customers in each segment\n",
    "    segment_counts = segment_data['segment'].value_counts().reset_index()\n",
    "    segment_counts.columns = ['segment', 'customer_count']\n",
    "    \n",
    "    return segment_counts\n",
    "\n",
    "# Analyze segments\n",
    "segment_counts = analyze_segments(segment_data)\n",
    "print(segment_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbe4993",
   "metadata": {},
   "source": [
    "# Content based filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf1248d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "def extract_features_from_product_name(product_name):\n",
    "    return product_name.lower().split()\n",
    "\n",
    "def extract_features_from_product_row(product_row):\n",
    "    keywords = product_row['keywords']\n",
    "    return keywords.lower().split()\n",
    "\n",
    "def calculate_similarity(features1, features2):\n",
    "    features1_set = set(features1)\n",
    "    features2_set = set(features2)\n",
    "    intersection = features1_set.intersection(features2_set)\n",
    "    union = features1_set.union(features2_set)\n",
    "    similarity_score = len(intersection) / len(union)\n",
    "    return similarity_score\n",
    "\n",
    "def content_based_filtering(customer_id, product_name, customer_data, product_data):\n",
    "    # Retrieve the products the customer has interacted with based on their purchase history\n",
    "    customer_interactions = customer_data.loc[customer_data['customer_id'] == customer_id, 'purchase_history'].iloc[0]\n",
    "\n",
    "    # Convert the purchase history to a list if it's a string\n",
    "    if isinstance(customer_interactions, str):\n",
    "        customer_interactions = ast.literal_eval(customer_interactions)\n",
    "\n",
    "    # Extract the attributes of the products the customer has interacted with\n",
    "    customer_product_attributes = product_data[product_data['product_name'].isin(customer_interactions)][['product_name', 'category', 'features', 'brand', 'keywords']]\n",
    "\n",
    "    # Use a simple example of recommending products from the same category\n",
    "    recommended_products = []\n",
    "\n",
    "    if not customer_product_attributes.empty:\n",
    "        category = customer_product_attributes.iloc[0]['category']\n",
    "        recommended_products.extend(product_data[product_data['category'] == category]['product_name'].tolist())\n",
    "\n",
    "    # Append product name-based recommendations\n",
    "    if product_name:\n",
    "        product_attributes = product_data[product_data['product_name'] == product_name][['product_name', 'category', 'features', 'brand', 'keywords']]\n",
    "        if not product_attributes.empty:\n",
    "            category = product_attributes.iloc[0]['category']\n",
    "            # Update feature extraction for product name\n",
    "            product_name_features = extract_features_from_product_name(product_name)\n",
    "            # Iterate through product data and calculate similarity with product name features\n",
    "            for index, row in product_data.iterrows():\n",
    "                row_features = extract_features_from_product_row(row)\n",
    "                similarity_score = calculate_similarity(product_name_features, row_features)\n",
    "                if similarity_score > 0:\n",
    "                    recommended_products.append(row['product_name'])\n",
    "\n",
    "    return recommended_products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd2d99b",
   "metadata": {},
   "source": [
    "# Collaboration based filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c97773b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def find_similar_customers(interactions_data, customer_id):\n",
    "    # Extract interaction data for the given customer\n",
    "    customer_interactions = interactions_data[interactions_data['customer_id'] == customer_id].drop_duplicates(subset=['product_id'])\n",
    "\n",
    "    # Extract interaction data for all customers\n",
    "    all_interactions = interactions_data.drop_duplicates(subset=['customer_id', 'product_id'])\n",
    "\n",
    "    # Calculate interaction count for each customer-product pair\n",
    "    interaction_counts = all_interactions.groupby(['customer_id', 'product_id']).size().reset_index(name='interaction_count')\n",
    "\n",
    "    # Merge data to have customer-product interaction matrix\n",
    "    interaction_matrix = pd.pivot_table(interaction_counts, values='interaction_count', index='customer_id', columns='product_id', fill_value=0)\n",
    "\n",
    "    # Extract interactions of similar customers\n",
    "    similar_customers = cosine_similarity(interaction_matrix.loc[[customer_id]], interaction_matrix)[0]\n",
    "    similar_customers_indices = np.argsort(similar_customers)[::-1][1:]  # Exclude the customer itself\n",
    "\n",
    "    # Return top similar customers\n",
    "    return interaction_matrix.index[similar_customers_indices]\n",
    "\n",
    "def collaborative_filtering(interactions_data, customer_data, product_data, customer_id, product_id, num_recommendations=5):\n",
    "    # Filter interactions data for the given customer\n",
    "    customer_interactions = interactions_data[(interactions_data['customer_id'] == customer_id) & (interactions_data['product_id'] != product_id)]\n",
    "    \n",
    "    # Find similar customers based on interactions\n",
    "    similar_customers = find_similar_customers(interactions_data, customer_id)\n",
    "    \n",
    "    # Aggregate interactions of similar customers\n",
    "    similar_customer_interactions = interactions_data[(interactions_data['customer_id'].isin(similar_customers)) & (interactions_data['product_id'] != product_id)]\n",
    "    aggregated_interactions = similar_customer_interactions.groupby('product_id').size().reset_index(name='interaction_count')\n",
    "    \n",
    "    # Recommend top N products\n",
    "    recommended_products = aggregated_interactions.sort_values(by='interaction_count', ascending=False).head(num_recommendations)['product_id'].tolist()\n",
    "    \n",
    "    return recommended_products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36ec6e1",
   "metadata": {},
   "source": [
    "# recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1540d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Recommendations: ['MacBook Pro', 'Samsung Galaxy S21', 'Lenovo ThinkPad', 'iPhone 13']\n"
     ]
    }
   ],
   "source": [
    "def get_user_input():\n",
    "    customer_id = input(\"Enter customer ID: \")\n",
    "    product_name = input(\"Enter product name: \")\n",
    "    return customer_id, product_name\n",
    "\n",
    "def hybrid_recommendation(content_based_recommendations, collaborative_recommendations):\n",
    "    # Combine recommendations from both approaches\n",
    "    hybrid_recommendations = list(set(content_based_recommendations + collaborative_recommendations))\n",
    "    return hybrid_recommendations\n",
    "\n",
    "# Example usage\n",
    "content_based_recommendations = ['iPhone 13', 'MacBook Pro']\n",
    "collaborative_recommendations = ['Samsung Galaxy S21', 'Lenovo ThinkPad']\n",
    "hybrid_recommendations = hybrid_recommendation(content_based_recommendations, collaborative_recommendations)\n",
    "print(\"Hybrid Recommendations:\", hybrid_recommendations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd9ae6e",
   "metadata": {},
   "source": [
    "# the number of increased recommendation which will increase purchase accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe979f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_input():\n",
    "    customer_id = input(\"Enter customer ID: \")\n",
    "    product_name = input(\"Enter product name: \")\n",
    "    return customer_id, product_name\n",
    "\n",
    "# Call get_user_input to get customer_id and product_name\n",
    "customer_id, product_name = get_user_input()\n",
    "\n",
    "initial_purchase_history = customer_data.loc[customer_data['customer_id'] == customer_id, 'purchase_history'].iloc[0]\n",
    "\n",
    "# Make Hybrid Recommendations (assuming content_based_filtering is defined elsewhere)\n",
    "hybrid_recommendations = content_based_filtering(customer_id, product_name, customer_data, product_data)\n",
    "\n",
    "# Compare States\n",
    "print(\"Initial Purchase History:\", initial_purchase_history)\n",
    "print(\"Hybrid Recommendations:\", hybrid_recommendations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3ef6b5",
   "metadata": {},
   "source": [
    "# Evaluating how well we did after applying recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec91fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recommendations(initial_purchase_history, recommended_products):\n",
    "    # Conversion Rate\n",
    "    initial_purchase_count = len(initial_purchase_history)\n",
    "    recommended_purchase_count = len(recommended_products)\n",
    "    conversion_rate = recommended_purchase_count / initial_purchase_count * 100\n",
    "    \n",
    "    # Average Number of Products Purchased per User\n",
    "    avg_products_purchased_per_user = recommended_purchase_count  # Assuming each recommended product is purchased once\n",
    "    \n",
    "    # Print or log the metrics\n",
    "    print(\"Conversion Rate:\", conversion_rate)\n",
    "    print(\"Average Products Purchased per User:\", avg_products_purchased_per_user)\n",
    "\n",
    "    # Return the metrics\n",
    "    return conversion_rate, avg_products_purchased_per_user\n",
    "\n",
    "# Usage\n",
    "initial_purchase_history = customer_data.loc[customer_data['customer_id'] == customer_id, 'purchase_history'].iloc[0]\n",
    "\n",
    "# Make Hybrid Recommendations\n",
    "hybrid_recommendations = content_based_filtering(customer_id, product_name, customer_data, product_data)\n",
    "\n",
    "# Evaluate the recommendations\n",
    "conversion_rate, avg_products_purchased_per_user = evaluate_recommendations(initial_purchase_history, hybrid_recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75d2330",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
